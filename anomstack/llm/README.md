# LLM Agent Integration

This directory contains the integration with AI agents for enhanced anomaly detection and intelligent alerting.

## Overview

The `llm` module provides integration with the [anomaly-agent](https://github.com/andrewm4894/anomaly-agent) project, enabling AI-powered anomaly analysis and contextual alerting. This represents Anomstack's "aGeNtIc" approach to anomaly detection.

## Components

### `agent.py`
- **Purpose**: LLM agent integration and orchestration
- **Functions**:
  - Interface with external AI agents for anomaly analysis
  - Process agent responses and recommendations
  - Handle agent authentication and API communication
  - Integrate agent insights into the alerting pipeline

## How LLM Agents Work

The LLM agent integration adds an intelligent layer on top of traditional ML-based anomaly detection:

1. **Traditional Detection**: PyOD algorithms detect statistical anomalies
2. **Agent Analysis**: LLM agent analyzes detected anomalies for context
3. **Intelligent Alerting**: Agent provides human-readable explanations
4. **Recommendation Generation**: Suggests potential causes and actions

## Agent Workflow

```
Anomaly Detected ‚Üí Agent Analysis ‚Üí Contextual Alert
       ‚Üì                ‚Üì              ‚Üì
   Statistical      AI Analysis    Human-Readable
   Anomaly Score    & Context      Explanation
```

## Configuration

LLM agent integration is configured through metric batch YAML files:

```yaml
# Enable LLM agent alerts
llm_alert_enabled: true

# Agent configuration
llm_agent_params:
  agent_url: "https://your-agent-endpoint.com"
  api_key: "${ANOMALY_AGENT_API_KEY}"
  model: "gpt-4"
  temperature: 0.1

# Alert settings
llm_alert_settings:
  include_context: true
  max_context_length: 2000
  analysis_timeout: 30
```

## Environment Variables

```bash
# Agent API configuration
ANOMALY_AGENT_API_KEY="your-agent-api-key"
ANOMALY_AGENT_URL="https://your-agent-endpoint.com"

# Optional: Custom model settings
ANOMALY_AGENT_MODEL="gpt-4"
ANOMALY_AGENT_TEMPERATURE="0.1"
```

## Agent Response Format

The LLM agent provides structured responses with:

```json
{
  "anomaly_detected": true,
  "confidence": 0.85,
  "explanation": "Revenue dropped 35% compared to last week's average...",
  "potential_causes": [
    "Weekend effect - lower weekend sales are normal",
    "Marketing campaign ended yesterday",
    "Possible system outage during peak hours"
  ],
  "recommended_actions": [
    "Check system logs for any outages",
    "Review marketing campaign performance",
    "Monitor closely for trend continuation"
  ],
  "severity": "medium",
  "context": {
    "metric_name": "daily_revenue",
    "current_value": 15000,
    "expected_range": [20000, 25000],
    "historical_pattern": "seasonal_weekly"
  }
}
```

## Integration with Alerts

LLM agent responses enhance traditional alerts:

### Traditional Alert
```
‚ö†Ô∏è ANOMALY DETECTED
Metric: daily_revenue
Current Value: 15,000
Anomaly Score: 0.85
Timestamp: 2023-12-15 10:00:00
```

### LLM-Enhanced Alert
```
ü§ñ AI ANOMALY ANALYSIS
Metric: daily_revenue
Current Value: 15,000 (35% below expected)
Confidence: 85%

üìä ANALYSIS
Revenue dropped significantly compared to last week's average. This could be due to:
‚Ä¢ Weekend effect - lower weekend sales are normal
‚Ä¢ Marketing campaign ended yesterday  
‚Ä¢ Possible system outage during peak hours

üîß RECOMMENDED ACTIONS
‚Ä¢ Check system logs for any outages
‚Ä¢ Review marketing campaign performance
‚Ä¢ Monitor closely for trend continuation

Severity: Medium | Generated by Anomaly Agent
```

## Benefits of LLM Integration

1. **Contextual Understanding**: Agents understand business context
2. **Reduced False Positives**: Intelligent filtering of insignificant anomalies
3. **Actionable Insights**: Specific recommendations for investigation
4. **Natural Language**: Human-readable explanations instead of just scores
5. **Learning Capability**: Agents can learn from feedback and patterns

## Use Cases

### Business Metrics
- **Revenue Anomalies**: Understand revenue drops in business context
- **User Behavior**: Explain unusual patterns in user engagement
- **Performance Metrics**: Contextualize system performance issues

### Technical Metrics
- **System Health**: Intelligent interpretation of infrastructure metrics
- **Error Rates**: Context-aware analysis of application errors
- **Resource Usage**: Smart analysis of CPU, memory, and disk usage

## Best Practices

1. **Set Appropriate Thresholds**: Don't send every anomaly to the agent
2. **Provide Context**: Include relevant metadata for better analysis
3. **Monitor Costs**: LLM API calls can be expensive for high-volume metrics
4. **Validate Responses**: Agent responses should be validated before acting
5. **Feedback Loop**: Provide feedback to improve agent performance

## Limitations

- **API Costs**: LLM API calls can be expensive for high-frequency metrics
- **Latency**: Agent analysis adds latency to alert generation
- **Accuracy**: Agent analysis is probabilistic and may be incorrect
- **Dependencies**: Requires external agent service availability

## Error Handling

The LLM integration includes robust error handling:

- **Fallback Alerts**: Send traditional alerts if agent fails
- **Timeout Handling**: Don't wait indefinitely for agent responses
- **Rate Limiting**: Respect API rate limits and quotas
- **Graceful Degradation**: Continue operation without agent if needed

## Monitoring

Monitor LLM agent performance:

```python
# Agent performance metrics
agent_response_time = timer.time()
agent_success_rate = successful_calls / total_calls
agent_cost_per_call = total_cost / total_calls
```

## Future Enhancements

- **Multi-Agent Support**: Use different agents for different metric types
- **Agent Training**: Fine-tune agents on your specific data patterns
- **Feedback Integration**: Learn from user feedback on agent recommendations
- **Cost Optimization**: Smart caching and batching to reduce API costs

## Troubleshooting

### Common Issues

1. **Agent Timeout**: Increase timeout or check agent availability
2. **High Costs**: Implement filtering to reduce unnecessary agent calls
3. **Poor Analysis**: Provide more context or adjust agent parameters
4. **API Errors**: Check authentication and API key validity

### Debugging

Enable LLM debugging:
```python
import logging
logging.getLogger('anomstack.llm').setLevel(logging.DEBUG)
```

This will log all agent interactions for troubleshooting.
